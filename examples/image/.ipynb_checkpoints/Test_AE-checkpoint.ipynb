{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np \n",
    "import argparse \n",
    "import time \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "from tc.tc_fc import TTLinear \n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size =200\n",
    "input_tensor = [7, 4, 7, 4]\n",
    "hidden_tensors= [[8, 4, 8, 4], [8, 4, 8, 4], [8, 4, 8, 8]]\n",
    "n_epochs = 5\n",
    "\n",
    "\n",
    "# class tt_model(nn.Module):\n",
    "#     def __init__(self, hidden_tensors, input_tensor, output_dim, tt_rank):\n",
    "#         super(tt_model, self).__init__()\n",
    "#         if len(hidden_tensors) != 3:\n",
    "#             raise ValueError('The depth of hidden layers should be 3!')\n",
    "\n",
    "#         self.TTLinear1 = TTLinear(input_tensor, hidden_tensors[0], tt_rank=tt_rank)\n",
    "#         self.TTLinear2 = TTLinear(hidden_tensors[0], hidden_tensors[1], tt_rank=tt_rank)\n",
    "#         self.TTLinear3 = TTLinear(hidden_tensors[1], hidden_tensors[2], tt_rank=tt_rank)\n",
    "#         self.fc4 = nn.Linear(np.prod(hidden_tensors[2]), output_dim)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         out = self.TTLinear1(inputs)\n",
    "#         out = self.TTLinear2(out)\n",
    "#         out = self.TTLinear3(out)\n",
    "#         out = self.fc4(out)\n",
    "\n",
    "#         return F.log_softmax(out, dim=1)\n",
    "\n",
    "class tt_autoencoder(nn.Module):\n",
    "    def __init__(self, hidden_tensors, input_tensor, output_dim, tt_rank):\n",
    "        super(tt_autoencoder, self).__init__()\n",
    "        self.encoder1 = TTLinear(input_tensor, hidden_tensors[0], tt_rank=tt_rank)\n",
    "        self.encoder2 = TTLinear(hidden_tensors[0], hidden_tensors[1], tt_rank=tt_rank)\n",
    "        self.encoder3 = TTLinear(hidden_tensors[1], hidden_tensors[2], tt_rank=tt_rank)\n",
    "        self.decoder1 = TTLinear(hidden_tensors[2],hidden_tensors[1], tt_rank=tt_rank),\n",
    "        self.decoder2 = TTLinear(hidden_tensors[1],hidden_tensors[0], tt_rank=tt_rank),\n",
    "        self.decoder3 = TTLinear(hidden_tensors[0],input_tensor, tt_rank=tt_rank)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ### Encoder layer\n",
    "        out = self.encoder1(inputs)\n",
    "        out = self.encoder2(out)\n",
    "#         out = self.encoder3(out)\n",
    "        ### Decoder Layer with activation\n",
    "#         out = self.decoder1(out)\n",
    "        out = self.decoder2(out)\n",
    "        out = F.sigmoid(self.decoder3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a Tensor-Train model...\n",
      "Model's state_dict:\n",
      "encoder1.b \t torch.Size([1])\n",
      "encoder1.W_cores.0 \t torch.Size([1, 7, 8, 2])\n",
      "encoder1.W_cores.1 \t torch.Size([2, 4, 4, 2])\n",
      "encoder1.W_cores.2 \t torch.Size([2, 7, 8, 2])\n",
      "encoder1.W_cores.3 \t torch.Size([2, 4, 4, 1])\n",
      "encoder2.b \t torch.Size([1])\n",
      "encoder2.W_cores.0 \t torch.Size([1, 8, 8, 2])\n",
      "encoder2.W_cores.1 \t torch.Size([2, 4, 4, 2])\n",
      "encoder2.W_cores.2 \t torch.Size([2, 8, 8, 2])\n",
      "encoder2.W_cores.3 \t torch.Size([2, 4, 4, 1])\n",
      "encoder3.b \t torch.Size([1])\n",
      "encoder3.W_cores.0 \t torch.Size([1, 8, 8, 2])\n",
      "encoder3.W_cores.1 \t torch.Size([2, 4, 4, 2])\n",
      "encoder3.W_cores.2 \t torch.Size([2, 8, 8, 2])\n",
      "encoder3.W_cores.3 \t torch.Size([2, 4, 8, 1])\n",
      "decoder3.b \t torch.Size([1])\n",
      "decoder3.W_cores.0 \t torch.Size([1, 8, 7, 2])\n",
      "decoder3.W_cores.1 \t torch.Size([2, 4, 4, 2])\n",
      "decoder3.W_cores.2 \t torch.Size([2, 8, 7, 2])\n",
      "decoder3.W_cores.3 \t torch.Size([2, 4, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "\n",
    "    ### get data\n",
    "    # convert data to torch.FloatTensor\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    # load the training and test datasets\n",
    "    train_data = datasets.MNIST(root='data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(root='data', train=False,\n",
    "                                      download=True, transform=transform)\n",
    "    # Create training and test dataloaders\n",
    "\n",
    "    # number of subprocesses to use for data loading\n",
    "    num_workers = 0\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 20\n",
    "\n",
    "    # prepare data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "    tt_rank = [1, 2, 2, 2, 1]\n",
    "    print('Building a Tensor-Train model...')\n",
    "    model = tt_autoencoder(hidden_tensors, input_tensor, 10, tt_rank).to(device)\n",
    "    \n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "    lr = 0.001\n",
    "    # specify loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # specify loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-prof/.pyenv/versions/anaconda3-5.3.1/envs/speech1/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 4.749967\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e912f92ccc24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/speech1/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/speech1/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        images, _ = data\n",
    "        # flatten images\n",
    "        images = images.view(images.size(0), -1)\n",
    "        images = images.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "\n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(),\"ae_tt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:speech1] *",
   "language": "python",
   "name": "conda-env-speech1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
